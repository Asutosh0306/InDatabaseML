{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmZcdpvfUdzr",
        "outputId": "6e145d53-2a75-43a5-fb8f-ccdc98cde21c"
      },
      "outputs": [],
      "source": [
        "# === Block 1: RAM logger ===\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "%pip -q install rpy2\n",
        "%load_ext rpy2.ipython\n",
        "\n",
        "import os, signal, subprocess, time\n",
        "from datetime import datetime\n",
        "\n",
        "LOG_PATH = \"/content/process_ram_usage.txt\"\n",
        "PID_PATH = \"/content/ram_logger.pid\"\n",
        "\n",
        "# Kill any existing logger we started earlier\n",
        "if os.path.exists(PID_PATH):\n",
        "    try:\n",
        "        with open(PID_PATH, \"r\") as f:\n",
        "            old_pid = int(f.read().strip())\n",
        "        os.kill(old_pid, signal.SIGTERM)\n",
        "        print(f\"Stopped previous RAM logger (PID {old_pid})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not stop previous logger: {e}\")\n",
        "    finally:\n",
        "        try: os.remove(PID_PATH)\n",
        "        except: pass\n",
        "\n",
        "# Fresh log file\n",
        "if os.path.exists(LOG_PATH):\n",
        "    os.remove(LOG_PATH)\n",
        "    print(\"Cleared existing RAM log file\")\n",
        "\n",
        "# Start a simple logger:\n",
        "bash_script = r'''\n",
        "set -e\n",
        "while true; do\n",
        "  {\n",
        "    echo \"---------------\"\n",
        "    echo \"TS $(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n",
        "    echo \"SYSTEM_TOTAL $(free -b | awk '/^Mem:/{print $2}')\"\n",
        "    echo \"SYSTEM_USED  $(free -b | awk '/^Mem:/{print $3}')\"\n",
        "    echo \"SYSTEM_AVAILABLE $(free -b | awk '/^Mem:/{print $7}')\"\n",
        "    ps -eo pid,user,comm,rss --sort=-rss | head -20 | awk '{print \"PROC\",$1,$2,$3,$4}'\n",
        "  } >> /content/process_ram_usage.txt\n",
        "  sleep 10\n",
        "done\n",
        "'''\n",
        "\n",
        "log_proc = subprocess.Popen([\"bash\", \"-c\", bash_script], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "with open(PID_PATH, \"w\") as f:\n",
        "    f.write(str(log_proc.pid))\n",
        "\n",
        "print(\"Background RAM logging started\")\n",
        "print(f\"Log file: {LOG_PATH}\")\n",
        "print(f\"Logger PID: {log_proc.pid}\")\n",
        "print(\"Stop later with: !kill -9 $(cat /content/ram_logger.pid)\")\n",
        "time.sleep(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QdjbQxMDUf4l",
        "outputId": "f114d26f-44ea-4f74-9913-4a10e0112625"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "# === Block 2: R training/eval with timing ===\n",
        "\n",
        "suppressPackageStartupMessages({\n",
        "  library(data.table)\n",
        "  library(rpart)\n",
        "})\n",
        "\n",
        "# Force UTC for all timestamps we capture\n",
        "Sys.setenv(TZ = \"UTC\")\n",
        "\n",
        "data <- list()  # list(process, start, end)\n",
        "\n",
        "log_start <- function(name) {\n",
        "  idx <- length(data) + 1L\n",
        "  data[[idx]] <<- list(process = name, start = as.POSIXct(Sys.time(), tz = \"UTC\"), end = as.POSIXct(NA))\n",
        "  invisible(idx)\n",
        "}\n",
        "log_end <- function(idx) {\n",
        "  data[[idx]]$end <<- as.POSIXct(Sys.time(), tz = \"UTC\")\n",
        "  invisible(NULL)\n",
        "}\n",
        "\n",
        "as_timing_df <- function(x = data) {\n",
        "  if (!length(x)) return(data.frame(process=character(), start=as.POSIXct(character()), end=as.POSIXct(character()), duration_secs=numeric()))\n",
        "  df <- data.table::rbindlist(lapply(x, as.data.table), fill = TRUE)\n",
        "  df[, duration_secs := as.numeric(difftime(end, start, units = \"secs\"))]\n",
        "  as.data.frame(df)\n",
        "}\n",
        "\n",
        "# --- Paths & schema (aligned with your latest setup) ---\n",
        "base_all   <- \"/content/drive/MyDrive/data\"\n",
        "base_final <- \"/content/drive/MyDrive/data\"\n",
        "\n",
        "train_raw_path <- file.path(base_all,   \"train.csv\")\n",
        "stores_path    <- file.path(base_all,   \"stores.csv\")\n",
        "items_path     <- file.path(base_all,   \"items.csv\")\n",
        "hol_path       <- file.path(base_all,   \"holidays_events.csv\")\n",
        "\n",
        "# Load test data\n",
        "test_path      <- file.path(base_final, \"df_test.csv\")\n",
        "\n",
        "features <- c(\n",
        "  \"perishable\",\"onpromotion\",\"on_hol\",\"month\",\"is_weekend\",\"day\",\n",
        "  \"store_nbr\",\"item_nbr\",\"city\",\"state\",\"type\",\"cluster\",\"family\",\"class\"\n",
        ")\n",
        "target <- \"unit_sales\"\n",
        "\n",
        "cat(\"=== Workflow start (UTC) ===\\n\")\n",
        "\n",
        "# Load training + meta (raw CSVs)\n",
        "i_load_tr <- log_start(\"load_train+meta\")\n",
        "stopifnot(file.exists(train_raw_path), file.exists(stores_path), file.exists(items_path), file.exists(hol_path))\n",
        "\n",
        "cat(\"Loading raw train/stores/items/holidays...\\n\")\n",
        "train  <- fread(train_raw_path,\n",
        "                select = c(\"id\",\"date\",\"store_nbr\",\"item_nbr\",\"unit_sales\",\"onpromotion\"),\n",
        "                showProgress = TRUE)\n",
        "stores <- fread(stores_path, select = c(\"store_nbr\",\"city\",\"state\",\"type\",\"cluster\"))\n",
        "items  <- fread(items_path,  select = c(\"item_nbr\",\"family\",\"class\",\"perishable\"))\n",
        "hol    <- fread(hol_path,    select = c(\"date\",\"locale\",\"locale_name\",\"type\",\"transferred\"))\n",
        "cat(sprintf(\"Train rows: %d | Stores: %d | Items: %d | Hol rows: %d\\n\",\n",
        "            nrow(train), nrow(stores), nrow(items), nrow(hol)))\n",
        "log_end(i_load_tr)\n",
        "\n",
        "# Load the previous test file but DO NOT use it below\n",
        "i_load_te <- log_start(\"load_test_keep_only\")\n",
        "if (file.exists(test_path)) {\n",
        "  cat(\"Loading test data\\n\")\n",
        "  DT_test_unused <- data.table::fread(test_path, showProgress = TRUE)\n",
        "  cat(sprintf(\"Kept test data: %d rows, %d columns\\n\", nrow(DT_test_unused), ncol(DT_test_unused)))\n",
        "} else {\n",
        "  cat(\"Test file not found; continuing without it.\\n\")\n",
        "}\n",
        "log_end(i_load_te)\n",
        "\n",
        "# Prep holidays (mirror Python logic)\n",
        "i_prep_hol <- log_start(\"prep_holidays\")\n",
        "cat(\"Preparing holiday/event flags...\\n\")\n",
        "hol[, transferred := tolower(as.character(transferred))]\n",
        "hol <- hol[is.na(transferred) | transferred != \"true\"]      # drop transferred\n",
        "hol <- hol[type != \"Work Day\"]                              # drop compensatory work days\n",
        "hol[, on_hol := fifelse(type %in% c(\"Holiday\",\"Bridge\",\"Additional\"), \"Holiday\", NA_character_)]\n",
        "hol[, on_evt := fifelse(type %in% c(\"Event\"), \"Event\", NA_character_)]\n",
        "\n",
        "locL <- hol[locale == \"Local\",    .(date, city  = locale_name, on_hol_L = on_hol, on_evt_L = on_evt)]\n",
        "locR <- hol[locale == \"Regional\", .(date, state = locale_name, on_hol_R = on_hol, on_evt_R = on_evt)]\n",
        "locN <- hol[locale == \"National\", .(date, on_hol_N = on_hol, on_evt_N = on_evt)]\n",
        "log_end(i_prep_hol)\n",
        "\n",
        "# Preprocess & merge like Python (allow cartesian joins; no duplicate checks)\n",
        "i_merge <- log_start(\"merge_join\")\n",
        "cat(\"Merging train + stores + items + (Local/Regional/National) holidays...\\n\")\n",
        "\n",
        "# ensure dates comparable\n",
        "train[, date := as.IDate(date)]\n",
        "locL[,  date := as.IDate(date)]\n",
        "locR[,  date := as.IDate(date)]\n",
        "locN[,  date := as.IDate(date)]\n",
        "\n",
        "DT <- merge(train,  stores, by = \"store_nbr\", all.x = TRUE, allow.cartesian = TRUE)\n",
        "DT <- merge(DT,     items,  by = \"item_nbr\",  all.x = TRUE, allow.cartesian = TRUE)\n",
        "DT <- merge(DT,     locL,   by = c(\"date\",\"city\"),  all.x = TRUE, allow.cartesian = TRUE)\n",
        "DT <- merge(DT,     locR,   by = c(\"date\",\"state\"), all.x = TRUE, allow.cartesian = TRUE)\n",
        "DT <- merge(DT,     locN,   by = \"date\",            all.x = TRUE, allow.cartesian = TRUE)\n",
        "\n",
        "# coalesce holiday/event flags (prefer any that are present)\n",
        "DT[, on_hol := fcoalesce(on_hol_L, on_hol_R, on_hol_N)]\n",
        "DT[, on_evt := fcoalesce(on_evt_L, on_evt_R, on_evt_N)]\n",
        "DT[, c(\"on_hol_L\",\"on_hol_R\",\"on_hol_N\",\"on_evt_L\",\"on_evt_R\",\"on_evt_N\") := NULL]\n",
        "\n",
        "cat(sprintf(\"Post-merge shape: %d rows, %d cols\\n\", nrow(DT), ncol(DT)))\n",
        "log_end(i_merge)\n",
        "\n",
        "# Feature engineering on the WHOLE merged table\n",
        "i_feat <- log_start(\"feature_engineering\")\n",
        "cat(\"Engineering features...\\n\")\n",
        "\n",
        "# target: clip returns -> 0\n",
        "DT[, unit_sales := pmax(0, as.numeric(unit_sales))]\n",
        "\n",
        "# promotions & perishability mappings\n",
        "DT[, onpromotion := as.character(onpromotion)]\n",
        "DT[, onpromotion := fifelse(onpromotion %in% c(\"True\",\"TRUE\",\"true\"), 1L,\n",
        "                      fifelse(onpromotion %in% c(\"False\",\"FALSE\",\"false\"), 0L, 2L))]\n",
        "DT[, perishable := fifelse(perishable == 0, 1.0,\n",
        "                      fifelse(perishable == 1, 1.25, 2.0))]\n",
        "\n",
        "# holiday flag: 'Holiday' -> 1 else -1\n",
        "DT[, on_hol := fifelse(on_hol == \"Holiday\", 1L, -1L)]\n",
        "\n",
        "# calendar features\n",
        "DT[, date := as.POSIXct(date, tz = \"UTC\")]\n",
        "DT[, month      := as.integer(format(date, \"%m\"))]\n",
        "DT[, is_weekend := as.integer(weekdays(date) %in% c(\"Saturday\",\"Sunday\"))]\n",
        "DT[, day        := weekdays(date)]\n",
        "\n",
        "# ensure categorical columns are factors\n",
        "cat_cols <- c(\"day\",\"city\",\"state\",\"type\",\"cluster\",\"family\",\"class\")\n",
        "for (col in cat_cols) {\n",
        "  if (!is.factor(DT[[col]])) DT[[col]] <- as.factor(DT[[col]])\n",
        "}\n",
        "\n",
        "# filter columns\n",
        "needed_cols <- unique(c(features, target))\n",
        "DT <- DT[, ..needed_cols]\n",
        "\n",
        "# quick sanity: all features exist\n",
        "miss_tr <- setdiff(c(features, target), names(DT))\n",
        "if (length(miss_tr)) stop(\"Merged table missing columns: \", paste(miss_tr, collapse = \", \"))\n",
        "\n",
        "log_end(i_feat)\n",
        "\n",
        "\n",
        "# Split 80/20 (training/test) from the engineered DT\n",
        "i_split <- log_start(\"split_train_test\")\n",
        "cat(\"Splitting 80/20 train/test...\\n\")\n",
        "set.seed(42)\n",
        "N <- nrow(DT)\n",
        "n_train <- floor(0.8 * N)\n",
        "idx <- sample.int(N, size = n_train)\n",
        "in_train <- logical(N); in_train[idx] <- TRUE\n",
        "\n",
        "TR <- DT[in_train]\n",
        "TE <- DT[!in_train]\n",
        "\n",
        "# ensure factor levels carried over (subsetting keeps levels; re-assert to be safe)\n",
        "for (col in cat_cols) {\n",
        "  if (is.factor(TR[[col]]) && is.factor(TE[[col]])) {\n",
        "    levels(TE[[col]]) <- levels(TR[[col]])\n",
        "  }\n",
        "}\n",
        "cat(sprintf(\"Train rows: %d | Test rows: %d\\n\", nrow(TR), nrow(TE)))\n",
        "log_end(i_split)\n",
        "\n",
        "# Train\n",
        "i_train <- log_start(\"train_model\")\n",
        "cat(\"Training model...\\n\")\n",
        "ctrl <- rpart.control(\n",
        "  minsplit = 1000, minbucket = 25, cp = 0.001,\n",
        "  maxdepth = 7, xval = 0, maxcompete = 0, maxsurrogate = 0, usesurrogate = 0\n",
        ")\n",
        "form <- as.formula(paste(target, \"~\", paste(features, collapse = \" + \")))\n",
        "set.seed(42)\n",
        "t0 <- Sys.time()\n",
        "model <- rpart(formula = form, data = TR, method = \"anova\", control = ctrl, model = FALSE, x = FALSE, y = FALSE)\n",
        "t1 <- Sys.time()\n",
        "cat(sprintf(\"Model trained in %.1f sec\\n\", as.numeric(difftime(t1, t0, units = \"secs\"))))\n",
        "log_end(i_train)\n",
        "\n",
        "# Predict on our 20% holdout\n",
        "i_pred <- log_start(\"predict\")\n",
        "cat(\"Predicting on 20%% holdout...\\n\")\n",
        "pred <- predict(model, newdata = TE[, features, with = FALSE])\n",
        "log_end(i_pred)\n",
        "\n",
        "# Evaluate\n",
        "i_eval <- log_start(\"evaluate\")\n",
        "cat(\"Evaluating...\\n\")\n",
        "y_true  <- as.numeric(TE[[target]])\n",
        "valid   <- !is.na(y_true) & !is.na(pred)\n",
        "y_true  <- y_true[valid]; y_pred <- pred[valid]\n",
        "mse  <- mean((y_true - y_pred)^2)\n",
        "rmse <- sqrt(mse)\n",
        "mae  <- mean(abs(y_true - y_pred))\n",
        "sse  <- sum((y_true - y_pred)^2)\n",
        "sst  <- sum((y_true - mean(y_true))^2)\n",
        "r2   <- 1 - (sse / sst)\n",
        "log_end(i_eval)\n",
        "\n",
        "# Results\n",
        "cat(\"\\nEVALUATION RESULTS\\n\")\n",
        "cat(sprintf(\"MSE : %.2f\\nRMSE: %.2f\\nMAE : %.2f\", mse, rmse, mae))\n",
        "\n",
        "# cat(\"\\n=== STEP TIMINGS (raw list) ===\\n\")\n",
        "# print(data)\n",
        "# cat(\"\\n--- Pretty view ---\\n\")\n",
        "# print(as_timing_df(data))\n",
        "# cat(\"\\n=== Workflow end ===\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZM3CADgU1jx",
        "outputId": "dd57394d-245c-4d1d-ed55-ab7024d2253d"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "# === Block 3: Export timing to CSV ===\n",
        "# Produces one row per (process, phase), where phase in {\"start\",\"end\"}.\n",
        "# Columns: process, phase, time\n",
        "\n",
        "stopifnot(exists(\"data\"))\n",
        "\n",
        "to_rows <- function(step) {\n",
        "  data.frame(\n",
        "    process = c(step$process, step$process),\n",
        "    phase   = c(\"start\", \"end\"),\n",
        "    time    = format(step$start, \"%Y-%m-%dT%H:%M:%SZ\", tz = \"UTC\")\n",
        "                |> c(format(step$end,   \"%Y-%m-%dT%H:%M:%SZ\", tz = \"UTC\")),\n",
        "    stringsAsFactors = FALSE\n",
        "  )\n",
        "}\n",
        "\n",
        "save_df <- do.call(rbind, lapply(data, to_rows))\n",
        "\n",
        "write.csv(save_df, file = \"/content/data.csv\", row.names = FALSE)\n",
        "cat(\"Saved timing data to /content/data.csv\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hpyrEMI2XMAa",
        "outputId": "2be75dcd-c42f-4b45-dc66-e45422d453f5"
      },
      "outputs": [],
      "source": [
        "# === Block 4: RAM plots - actual and normalized time ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# ---- Inputs ----\n",
        "TIMING_CSV = \"/content/data.csv\"                 # written by Block 3\n",
        "LOG_PATH   = \"/content/process_ram_usage.txt\"    # written by Block 1\n",
        "\n",
        "# ---- Output base (per-run folder) ----\n",
        "RUNS_BASE = \"/content/drive/MyDrive/runs\"\n",
        "os.makedirs(RUNS_BASE, exist_ok=True)\n",
        "\n",
        "# ---- Fixed segment colors (blue = loading, green = join, yellow = training, red = eval) ----\n",
        "COLOR_LOADING  = \"#1f77b4\"  # blue\n",
        "COLOR_JOIN     = \"#2ca02c\"  # green\n",
        "COLOR_TRAINING = \"#ffd700\"  # yellow (gold)\n",
        "COLOR_EVAL     = \"#d62728\"  # red\n",
        "\n",
        "# ---- Segment layout for normalized plot ----\n",
        "SEG_WIDTHS = {\"loading\": 0.20, \"join\": 0.20, \"training\": 0.40, \"evaluate\": 0.20}\n",
        "SEG_ORDER  = [\"loading\", \"join\", \"training\", \"evaluate\"]\n",
        "SEG_COLORS = {\n",
        "    \"loading\":  COLOR_LOADING,\n",
        "    \"join\":     COLOR_JOIN,\n",
        "    \"training\": COLOR_TRAINING,\n",
        "    \"evaluate\": COLOR_EVAL,\n",
        "}\n",
        "\n",
        "# 1) Load workflow events (expects: process, phase, time)\n",
        "events = pd.read_csv(TIMING_CSV)\n",
        "events[\"time\"] = pd.to_datetime(events[\"time\"], utc=True)\n",
        "\n",
        "first_start = events.loc[events[\"phase\"] == \"start\", \"time\"].min()\n",
        "if pd.isna(first_start):\n",
        "    raise RuntimeError(\"No start event found in timing CSV.\")\n",
        "\n",
        "# Stable run_id per run\n",
        "run_id = \"run_\" + first_start.strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "run_dir = os.path.join(RUNS_BASE, run_id)\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "def _first_end(process_name):\n",
        "    r = events[(events[\"process\"] == process_name) & (events[\"phase\"] == \"end\")]\n",
        "    return pd.to_datetime(r.iloc[0][\"time\"], utc=True) if not r.empty else pd.NaT\n",
        "\n",
        "def _max_end(mask):\n",
        "    r = events[mask & (events[\"phase\"] == \"end\")]\n",
        "    return pd.to_datetime(r[\"time\"], utc=True).max() if not r.empty else pd.NaT\n",
        "\n",
        "# Flexible loaders (load_train, load_train+meta, load_test_keep_only, load_test, etc.)\n",
        "after_load = _max_end(events[\"process\"].str.startswith(\"load\", na=False))\n",
        "\n",
        "# Flexible join/merge\n",
        "join_mask = events[\"process\"].str.contains(\"merge|join\", case=False, regex=True, na=False)\n",
        "after_join = _max_end(join_mask)\n",
        "\n",
        "# Train & eval\n",
        "after_train = _first_end(\"train_model\")\n",
        "after_eval  = _first_end(\"evaluate\")\n",
        "\n",
        "# Build markers\n",
        "markers = [{\"label\": \"before_load\", \"time\": first_start}]\n",
        "if not pd.isna(after_load):\n",
        "    markers.append({\"label\": \"after_load\", \"time\": after_load})\n",
        "if not pd.isna(after_join):\n",
        "    markers.append({\"label\": \"after_join\", \"time\": after_join})\n",
        "if not pd.isna(after_train):\n",
        "    markers.append({\"label\": \"after_train\", \"time\": after_train})\n",
        "if not pd.isna(after_eval):\n",
        "    markers.append({\"label\": \"after_eval\", \"time\": after_eval})\n",
        "\n",
        "markers_df = pd.DataFrame(markers).sort_values(\"time\").reset_index(drop=True)\n",
        "\n",
        "# Quick diagnostics so you can see what was detected\n",
        "print(\"== Detected milestones ==\")\n",
        "for m in markers:\n",
        "    print(f\"{m['label']:>12}: {m['time']}\")\n",
        "\n",
        "# 2) Parse RAM log (SYSTEM_USED bytes → GB)\n",
        "def parse_ram_log(path=LOG_PATH):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"RAM log not found: {path}\")\n",
        "    ts = None\n",
        "    rows = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if s.startswith(\"TS \"):\n",
        "                ts = pd.to_datetime(s.split()[1], utc=True)\n",
        "            elif s.startswith(\"SYSTEM_USED\"):\n",
        "                try:\n",
        "                    used_bytes = int(s.split()[1])\n",
        "                    if ts is not None:\n",
        "                        rows.append({\"time\": ts, \"ram_gb\": used_bytes / (1024**3)})\n",
        "                except Exception:\n",
        "                    pass\n",
        "    df = pd.DataFrame(rows).sort_values(\"time\").reset_index(drop=True)\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Parsed RAM log is empty. Has the logger run long enough?\")\n",
        "    return df\n",
        "\n",
        "ram = parse_ram_log()\n",
        "ram[\"time\"] = pd.to_datetime(ram[\"time\"], utc=True)\n",
        "\n",
        "# 3) Align milestones to nearest sample at-or-before\n",
        "markers_for_merge = markers_df.rename(columns={\"time\": \"event_time\"}).sort_values(\"event_time\")\n",
        "aligned_df = pd.merge_asof(\n",
        "    markers_for_merge, ram.sort_values(\"time\"),\n",
        "    left_on=\"event_time\", right_on=\"time\", direction=\"backward\"\n",
        ").rename(columns={\"time\":\"plot_time\"})[[\"label\",\"event_time\",\"plot_time\",\"ram_gb\"]]\n",
        "aligned_df = aligned_df.dropna(subset=[\"plot_time\"]).reset_index(drop=True)\n",
        "\n",
        "m_time = {r.label: r.plot_time for _, r in aligned_df.iterrows()}\n",
        "m_y    = {r.label: r.ram_gb    for _, r in aligned_df.iterrows()}\n",
        "\n",
        "# 4) Consolidated segments\n",
        "SEGMENTS = [\n",
        "    (\"loading\",  \"before_load\", \"after_load\"),\n",
        "    (\"join\",     \"after_load\",  \"after_join\"),\n",
        "    (\"training\", \"after_join\",  \"after_train\"),\n",
        "    (\"evaluate\", \"after_train\", \"after_eval\"),\n",
        "]\n",
        "\n",
        "# Helpers\n",
        "def interp_ram_at(t, times, values):\n",
        "    if t <= times.iloc[0]:  return float(values.iloc[0])\n",
        "    if t >= times.iloc[-1]: return float(values.iloc[-1])\n",
        "    idx = times.searchsorted(t, side=\"left\")\n",
        "    t0, t1 = times.iloc[idx-1], times.iloc[idx]\n",
        "    y0, y1 = values.iloc[idx-1], values.iloc[idx]\n",
        "    frac = (t - t0) / (t1 - t0)\n",
        "    return float(y0 + frac * (y1 - y0))\n",
        "\n",
        "def build_segment_series(t_start, t_end, ram_df):\n",
        "    ram_df = ram_df.sort_values(\"time\").reset_index(drop=True)\n",
        "    if (pd.isna(t_start)) or (pd.isna(t_end)) or (t_end <= t_start):\n",
        "        return pd.DataFrame(columns=[\"time\",\"ram_gb\"])\n",
        "    inside = ram_df[(ram_df[\"time\"] > t_start) & (ram_df[\"time\"] < t_end)].copy()\n",
        "    y_start = interp_ram_at(t_start, ram_df[\"time\"], ram_df[\"ram_gb\"])\n",
        "    y_end   = interp_ram_at(t_end,   ram_df[\"time\"], ram_df[\"ram_gb\"])\n",
        "    out = pd.concat([\n",
        "        pd.DataFrame([{\"time\": t_start, \"ram_gb\": y_start}]),\n",
        "        inside[[\"time\",\"ram_gb\"]],\n",
        "        pd.DataFrame([{\"time\": t_end,   \"ram_gb\": y_end}]),\n",
        "    ], ignore_index=True).sort_values(\"time\").reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "def segment_stats(df):\n",
        "    if len(df) < 2:\n",
        "        return dict(mean=np.nan, peak=df[\"ram_gb\"].max() if not df.empty else np.nan,\n",
        "                    duration_secs=0.0, samples=len(df))\n",
        "    t = df[\"time\"].astype(\"int64\").to_numpy() / 1e9  # seconds\n",
        "    y = df[\"ram_gb\"].to_numpy()\n",
        "    duration = t[-1] - t[0]\n",
        "    area = np.trapezoid(y, t)  # np.trapz is deprecated\n",
        "    mean = area / duration if duration > 0 else np.nan\n",
        "    peak = float(np.max(y))\n",
        "    return dict(mean=mean, peak=peak, duration_secs=duration, samples=len(df))\n",
        "\n",
        "# 5) Build series & stats\n",
        "seg_series, seg_stats = {}, {}\n",
        "for seg_name, start_lab, end_lab in SEGMENTS:\n",
        "    if (start_lab in m_time) and (end_lab in m_time):\n",
        "        t0, t1 = m_time[start_lab], m_time[end_lab]\n",
        "        s = build_segment_series(t0, t1, ram)\n",
        "        if not s.empty:\n",
        "            seg_series[seg_name] = s\n",
        "            seg_stats[seg_name] = segment_stats(s)\n",
        "\n",
        "avail_labels = [k for k in [\"before_load\",\"after_load\",\"after_join\",\"after_train\",\"after_eval\"] if k in m_time]\n",
        "avail_times = [m_time[k] for k in avail_labels]\n",
        "if len(avail_times) >= 2:\n",
        "    run_start, run_end = min(avail_times), max(avail_times)\n",
        "else:\n",
        "    run_start = first_start\n",
        "    run_end   = ram[\"time\"].iloc[-1]\n",
        "run_series = build_segment_series(run_start, run_end, ram)\n",
        "overall_stats = segment_stats(run_series)\n",
        "\n",
        "# 6) Plot 1 : Actual time (UTC)\n",
        "fig1, ax1 = plt.subplots(figsize=(14, 6))\n",
        "ax1.plot(ram[\"time\"], ram[\"ram_gb\"], linewidth=2, alpha=0.95, zorder=3)\n",
        "y_min, y_max = ram[\"ram_gb\"].min(), ram[\"ram_gb\"].max()\n",
        "y_text = y_min + 0.05 * (y_max - y_min)\n",
        "\n",
        "for seg_name, start_lab, end_lab in SEGMENTS:\n",
        "    if seg_name in seg_series:\n",
        "        df = seg_series[seg_name]\n",
        "        ax1.fill_between(df[\"time\"], 0, df[\"ram_gb\"], color=SEG_COLORS[seg_name], alpha=0.28, zorder=1)\n",
        "        x0, x1 = df[\"time\"].iloc[0], df[\"time\"].iloc[-1]\n",
        "        y0, y1 = df[\"ram_gb\"].iloc[0], df[\"ram_gb\"].iloc[-1]\n",
        "        ax1.scatter([x0, x1], [y0, y1], s=80, zorder=4, color=SEG_COLORS[seg_name], edgecolors=\"black\")\n",
        "        label = {\"loading\":\"Loading\",\"join\":\"Join\",\"training\":\"Training\",\"evaluate\":\"Evaluate\"}[seg_name]\n",
        "        x_mid = x0 + (x1 - x0) / 2\n",
        "        ax1.text(x_mid, y_text, label, ha=\"center\", va=\"center\",\n",
        "                 fontsize=10, fontweight=\"bold\",\n",
        "                 bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.6),\n",
        "                 zorder=5)\n",
        "ax1.set_title(\"Memory Profile: RAPRT (DecisionTreeRegressor) — Join of Tables in Memory (Full Dataset)\", fontweight=\"bold\")\n",
        "ax1.set_xlabel(\"Time (UTC)\")\n",
        "ax1.set_ylabel(\"Memory Usage (GB)\")\n",
        "ax1.grid(True, alpha=0.3, linestyle=\"--\")\n",
        "ax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\", tz=mdates.UTC))\n",
        "fig1.autofmt_xdate()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 7) Normalized data + Plot 2 : Normalized time\n",
        "seg_spans, cursor = {}, 0.0\n",
        "for name in SEG_ORDER:\n",
        "    w = SEG_WIDTHS[name]\n",
        "    seg_spans[name] = (cursor, cursor + w)\n",
        "    cursor += w\n",
        "\n",
        "norm_rows = []\n",
        "for seg_name in SEG_ORDER:\n",
        "    if seg_name not in seg_series:\n",
        "        continue\n",
        "    df = seg_series[seg_name]\n",
        "    t0, t1 = df[\"time\"].iloc[0], df[\"time\"].iloc[-1]\n",
        "    denom_ns = (t1.value - t0.value)\n",
        "    if denom_ns <= 0:\n",
        "        continue\n",
        "    times_ns = df[\"time\"].astype(\"int64\").to_numpy()\n",
        "    x_seg = (times_ns - t0.value) / float(denom_ns)  # [0,1] within segment\n",
        "    span0, span1 = seg_spans[seg_name]\n",
        "    x_global = span0 + x_seg * (span1 - span0)\n",
        "    for xs, xg, y, tt in zip(x_seg, x_global, df[\"ram_gb\"].to_numpy(), df[\"time\"]):\n",
        "        norm_rows.append({\n",
        "            \"run_id\": run_id, \"segment\": seg_name,\n",
        "            \"x_segment_norm\": float(xs), \"x_global_norm\": float(xg),\n",
        "            \"ram_gb\": float(y), \"time_utc\": tt.isoformat()\n",
        "        })\n",
        "norm_df = pd.DataFrame(norm_rows).sort_values([\"x_global_norm\"]).reset_index(drop=True)\n",
        "\n",
        "# Save minimal per-run data for super plot\n",
        "norm_csv_path = os.path.join(run_dir, \"ram_profile_normalized.csv\")\n",
        "norm_df.to_csv(norm_csv_path, index=False)\n",
        "\n",
        "# Plot 2 : Normalized \n",
        "fig2, ax2 = plt.subplots(figsize=(14, 6))\n",
        "if not norm_df.empty:\n",
        "    ax2.plot(norm_df[\"x_global_norm\"], norm_df[\"ram_gb\"], linewidth=2, alpha=0.95, zorder=3)\n",
        "    for seg_name in SEG_ORDER:\n",
        "        span = seg_spans[seg_name]\n",
        "        df_seg = norm_df[norm_df[\"segment\"] == seg_name]\n",
        "        if df_seg.empty:\n",
        "            continue\n",
        "        ax2.fill_between(df_seg[\"x_global_norm\"], 0, df_seg[\"ram_gb\"],\n",
        "                         color=SEG_COLORS[seg_name], alpha=0.28, zorder=1)\n",
        "        x0, x1 = span\n",
        "        # pick nearest points to the boundaries for markers\n",
        "        y0 = df_seg.iloc[(df_seg[\"x_global_norm\"]-x0).abs().argmin()][\"ram_gb\"]\n",
        "        y1 = df_seg.iloc[(df_seg[\"x_global_norm\"]-x1).abs().argmin()][\"ram_gb\"]\n",
        "        ax2.scatter([x0, x1], [y0, y1], s=80, zorder=4, color=SEG_COLORS[seg_name], edgecolors=\"black\")\n",
        "        label = {\"loading\":\"Loading\",\"join\":\"Join\",\"training\":\"Training\",\"evaluate\":\"Evaluate\"}[seg_name]\n",
        "        x_mid = x0 + (x1 - x0) / 2\n",
        "        y_min2, y_max2 = norm_df[\"ram_gb\"].min(), norm_df[\"ram_gb\"].max()\n",
        "        y_text2 = y_min2 + 0.05 * (y_max2 - y_min2)\n",
        "        ax2.text(x_mid, y_text2, label, ha=\"center\", va=\"center\",\n",
        "                 fontsize=10, fontweight=\"bold\",\n",
        "                 bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.6),\n",
        "                 zorder=5)\n",
        "ax2.set_xlim(0.0, 1.0)\n",
        "ax2.set_title(\"Memory Profile: RAPRT (DecisionTreeRegressor) — Join of Tables in Memory (Full Dataset)\", fontweight=\"bold\")\n",
        "ax2.set_xlabel(\"Normalized Time\")\n",
        "ax2.set_ylabel(\"Memory Usage (GB)\")\n",
        "ax2.grid(True, alpha=0.3, linestyle=\"--\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 8) Print and save summary stats (overall + per segment)\n",
        "def pretty_stats(name, st):\n",
        "    print(f\"{name:>10}: mean={st['mean']:.3f} GB | peak={st['peak']:.3f} GB | duration={st['duration_secs']:.1f}s | samples={st['samples']}\")\n",
        "print(\"\\n=== RAM SUMMARY (time-weighted) ===\")\n",
        "pretty_stats(\"OVERALL\", overall_stats)\n",
        "for seg_name in SEG_ORDER:\n",
        "    if seg_name in seg_stats:\n",
        "        title = {\"loading\":\"LOADING\",\"join\":\"JOIN\",\"training\":\"TRAINING\",\"evaluate\":\"EVALUATE\"}[seg_name]\n",
        "        pretty_stats(title, seg_stats[seg_name])\n",
        "\n",
        "summary_rows = [{\n",
        "    \"run_id\": run_id, \"scope\": \"overall\",\n",
        "    \"mean_ram_gb\": overall_stats[\"mean\"], \"peak_ram_gb\": overall_stats[\"peak\"],\n",
        "    \"duration_secs\": overall_stats[\"duration_secs\"], \"samples\": overall_stats[\"samples\"]\n",
        "}]\n",
        "for seg_name in SEG_ORDER:\n",
        "    if seg_name in seg_stats:\n",
        "        summary_rows.append({\n",
        "            \"run_id\": run_id, \"scope\": seg_name,\n",
        "            \"mean_ram_gb\": seg_stats[seg_name][\"mean\"], \"peak_ram_gb\": seg_stats[seg_name][\"peak\"],\n",
        "            \"duration_secs\": seg_stats[seg_name][\"duration_secs\"], \"samples\": seg_stats[seg_name][\"samples\"]\n",
        "        })\n",
        "pd.DataFrame(summary_rows).to_csv(os.path.join(run_dir, \"run_summary.csv\"), index=False)\n",
        "print(f\"\\n Saved normalized plot data: {norm_csv_path}\")\n",
        "print(f\"Saved summary stats:       {os.path.join(run_dir, 'run_summary.csv')}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
