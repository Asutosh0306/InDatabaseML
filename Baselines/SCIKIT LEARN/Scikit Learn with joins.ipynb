{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50qmAXfllYDy",
        "outputId": "51e6adc8-0a6a-4fcb-ba11-bdc49e471ccd"
      },
      "outputs": [],
      "source": [
        "# === Block 1: RAM logger ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, signal, subprocess, time\n",
        "\n",
        "LOG_PATH = \"/content/process_ram_usage.txt\"\n",
        "PID_PATH = \"/content/ram_logger.pid\"\n",
        "\n",
        "# Kill any existing logger we started earlier\n",
        "if os.path.exists(PID_PATH):\n",
        "    try:\n",
        "        with open(PID_PATH, \"r\") as f:\n",
        "            old_pid = int(f.read().strip())\n",
        "        os.kill(old_pid, signal.SIGTERM)\n",
        "        print(f\"Stopped previous RAM logger (PID {old_pid})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not stop previous logger: {e}\")\n",
        "    finally:\n",
        "        try: os.remove(PID_PATH)\n",
        "        except: pass\n",
        "\n",
        "# Fresh log file\n",
        "if os.path.exists(LOG_PATH):\n",
        "    os.remove(LOG_PATH)\n",
        "    print(\"Cleared existing RAM log file\")\n",
        "\n",
        "# Start a simple logger:\n",
        "bash_script = r'''\n",
        "set -e\n",
        "while true; do\n",
        "  {\n",
        "    echo \"---------------\"\n",
        "    echo \"TS $(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n",
        "    echo \"SYSTEM_TOTAL $(free -b | awk '/^Mem:/{print $2}')\"\n",
        "    echo \"SYSTEM_USED  $(free -b | awk '/^Mem:/{print $3}')\"\n",
        "    echo \"SYSTEM_AVAILABLE $(free -b | awk '/^Mem:/{print $7}')\"\n",
        "    ps -eo pid,user,comm,rss --sort=-rss | head -20 | awk '{print \"PROC\",$1,$2,$3,$4}'\n",
        "  } >> /content/process_ram_usage.txt\n",
        "  sleep 10\n",
        "done\n",
        "'''\n",
        "\n",
        "log_proc = subprocess.Popen([\"bash\", \"-c\", bash_script], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "with open(PID_PATH, \"w\") as f:\n",
        "    f.write(str(log_proc.pid))\n",
        "\n",
        "print(\"Background RAM logging started\")\n",
        "print(f\"Log file: {LOG_PATH}\")\n",
        "print(f\"Logger PID: {log_proc.pid}\")\n",
        "print(\"Stop later with: !kill -9 $(cat /content/ram_logger.pid)\")\n",
        "time.sleep(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQTskz8hldLE",
        "outputId": "cbe8c761-d4a2-489d-d096-4fa00127075a"
      },
      "outputs": [],
      "source": [
        "# === Block 2: Training/eval scikit-learn Decision Tree ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# --- Timing helpers (UTC) ---\n",
        "data = []  # list of dict(process, start, end)\n",
        "\n",
        "\n",
        "def utcnow():\n",
        "    return datetime.now(timezone.utc)\n",
        "\n",
        "\n",
        "def log_start(name: str) -> int:\n",
        "    idx = len(data)\n",
        "    data.append({\"process\": name, \"start\": utcnow(), \"end\": None})\n",
        "    return idx\n",
        "\n",
        "\n",
        "def log_end(idx: int):\n",
        "    data[idx][\"end\"] = utcnow()\n",
        "\n",
        "\n",
        "def as_timing_df():\n",
        "    if not data:\n",
        "        return pd.DataFrame(columns=[\"process\",\"start\",\"end\",\"duration_secs\"])\n",
        "    df = pd.DataFrame(data)\n",
        "    df[\"duration_secs\"] = (df[\"end\"] - df[\"start\"]).dt.total_seconds()\n",
        "    return df\n",
        "\n",
        "\n",
        "print(\"=== Workflow start (UTC) ===\")\n",
        "\n",
        "i_load_tr = log_start(\"load_train\")\n",
        "# 1. Load raw data (only finalized columns) -------------------------------\n",
        "dtypes = {'id':'int64', 'item_nbr':'int32', 'store_nbr':'int8', 'onpromotion':'str'}\n",
        "train = pd.read_csv('/content/drive/MyDrive/Dissertation/all files/train.csv',\n",
        "                    dtype=dtypes,\n",
        "                    parse_dates=['date'],\n",
        "                    usecols=['id','date','store_nbr','item_nbr','unit_sales','onpromotion'])\n",
        "\n",
        "log_end(i_load_tr)\n",
        "\n",
        "stores = pd.read_csv('/content/drive/MyDrive/Dissertation/all files/stores.csv',\n",
        "                     usecols=['store_nbr','city','state','type','cluster'])\n",
        "\n",
        "\n",
        "items = pd.read_csv('/content/drive/MyDrive/Dissertation/all files/items.csv',\n",
        "                    usecols=['item_nbr','family','class','perishable'])\n",
        "\n",
        "\n",
        "hol = pd.read_csv('/content/drive/MyDrive/Dissertation/all files/holidays_events.csv',\n",
        "                  dtype={'transferred':'str'},\n",
        "                  parse_dates=['date'],\n",
        "                  usecols=['date','locale','locale_name','type','transferred'])\n",
        "\n",
        "\n",
        "log_end(i_load_tr)\n",
        "\n",
        "# --- Paths & schema ---\n",
        "base_path  = \"/content/drive/MyDrive/Dissertation/final\"\n",
        "test_path  = f\"{base_path}/df_test.csv\"\n",
        "\n",
        "# Load test\n",
        "i_load_te = log_start(\"load_test\")\n",
        "assert os.path.exists(test_path), f\"Test file not found: {test_path}\"\n",
        "print(\"Loading test data...\")\n",
        "DT_test = pd.read_csv(test_path, low_memory=False)\n",
        "print(f\"Test data loaded: {len(DT_test)} rows, {DT_test.shape[1]} columns\")\n",
        "log_end(i_load_te)\n",
        "\n",
        "\n",
        "\n",
        "print(\"start join\")\n",
        "i_join = log_start(\"join\")\n",
        "\n",
        "# 2. Prepare holiday/event flags ------------------------------------------\n",
        "hol = (\n",
        "    hol[~hol.transferred.str.lower().eq('true')]       # drop transferred\n",
        "       .query(\"type!='Work Day'\")                       # drop compensatory work days\n",
        "       .assign(\n",
        "           on_hol=lambda df: df.type.map({\n",
        "               'Holiday':'Holiday','Bridge':'Holiday','Additional':'Holiday'\n",
        "           }),\n",
        "           on_evt=lambda df: df.type.map({'Event':'Event'})\n",
        "       )\n",
        ")\n",
        "locL = (\n",
        "    hol.query(\"locale=='Local'\")\n",
        "       .loc[:, ['date','locale_name','on_hol','on_evt']]\n",
        "       .rename(columns={'locale_name':'city'})\n",
        ")\n",
        "locR = (\n",
        "    hol.query(\"locale=='Regional'\")\n",
        "       .loc[:, ['date','locale_name','on_hol','on_evt']]\n",
        "       .rename(columns={'locale_name':'state'})\n",
        ")\n",
        "locN = hol.query(\"locale=='National'\")[['date','on_hol','on_evt']]\n",
        "\n",
        "\n",
        "\n",
        "# 3. Merge into single DataFrame -----------------------------------------\n",
        "\n",
        "df = (\n",
        "    train\n",
        "      .merge(stores, on='store_nbr', how='left')\n",
        "      .merge(items,  on='item_nbr',  how='left')\n",
        "      .merge(locL,   on=['date','city'],  how='left')\n",
        "      .merge(locR,   on=['date','state'], how='left')\n",
        "      .merge(locN,   on='date',           how='left')\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# 4. Keep only final features --------------------------------------------\n",
        "df = df[[\n",
        "    'id','unit_sales','date','store_nbr','item_nbr',\n",
        "    'city','state','type','cluster','family','class','perishable',\n",
        "    'onpromotion','on_hol','on_evt'\n",
        "]].copy()\n",
        "\n",
        "\n",
        "# 5. Basic transformations -----------------------------------------------\n",
        "\n",
        "df['unit_sales'] = df['unit_sales'].clip(lower=0)\n",
        "\n",
        "\n",
        "# calendar features\n",
        "df['month']       = df['date'].dt.month\n",
        "df['wage']        = df['date'].dt.day.isin([15, 31]).astype(int)\n",
        "df['is_weekend']  = (df['date'].dt.dayofweek >= 5).astype(int)\n",
        "\n",
        "\n",
        "# promotions & perishability\n",
        "df['onpromotion'] = df['onpromotion'].map({'False': 0, 'True': 1}).fillna(2).astype(int)\n",
        "df['perishable']  = df['perishable'].map({0: 1.0, 1: 1.25}).fillna(2)\n",
        "\n",
        "\n",
        "# holiday/event flags\n",
        "# prefer map+fillna so the result is a pandas Series\n",
        "df['on_hol'] = df['on_hol'].map({'Holiday': 1}).fillna(-1).astype(int)\n",
        "df['on_evt'] = df['on_evt'].map({'Event':   1}).fillna(-1).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert 'date' column to datetime\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "\n",
        "# Create 'day' column with weekday names\n",
        "df['day'] = df['date'].dt.day_name()\n",
        "\n",
        "\n",
        "\n",
        "# 7. Target (mean/rank) encode categoricals\n",
        "categorical_cols = [\n",
        "    'store_nbr','item_nbr','city','state',\n",
        "    'type','cluster','family','class', 'day'\n",
        "]\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=np.uint8)\n",
        "X_ohe = ohe.fit_transform(df[categorical_cols])\n",
        "\n",
        "ohe_cols = ohe.get_feature_names_out(categorical_cols)\n",
        "df_ohe = pd.DataFrame(X_ohe, columns=ohe_cols, index=df.index)\n",
        "\n",
        "df = pd.concat([df.drop(columns=categorical_cols), df_ohe], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Create df_final with only the required features (dropping raw categoricals & outlier stats)\n",
        "df_final = df[[\n",
        "    'id',\n",
        "    'unit_sales',\n",
        "    'date',\n",
        "    'perishable',\n",
        "    'onpromotion',\n",
        "    'on_hol',\n",
        "    'on_evt',\n",
        "    'month',\n",
        "    'wage',\n",
        "    'is_weekend',\n",
        "    'day',\n",
        "    'store_nbr',\n",
        "    'item_nbr',\n",
        "    'city',\n",
        "    'state',\n",
        "    'type',\n",
        "    'cluster',\n",
        "    'family',\n",
        "    'class'\n",
        "]]\n",
        "\n",
        "\n",
        "\n",
        "features = [\n",
        "  \"perishable\",\"onpromotion\",\"on_hol\",\"month\",\"is_weekend\",\"day\",\n",
        "  \"store_nbr\",\"item_nbr\",\"city\",\"state\",\"type\",\"cluster\",\"family\",\"class\"\n",
        "]\n",
        "target = \"unit_sales\"\n",
        "log_end(i_join)\n",
        "\n",
        "# --- Prepare data ---\n",
        "i_prep = log_start(\"prepare_data\")\n",
        "print(\"Preparing data...\")\n",
        "\n",
        "\n",
        "X_train = df_final[features].copy()\n",
        "y_train = df_final[target].copy()\n",
        "X_test  = df_final[features].copy()\n",
        "y_test  = df_final[target].copy()\n",
        "\n",
        "\n",
        "log_end(i_prep)\n",
        "\n",
        "\n",
        "# --- Model ---\n",
        "model = DecisionTreeRegressor(\n",
        "    max_depth=7,\n",
        "    min_samples_split=1000\n",
        ")\n",
        "\n",
        "\n",
        "# --- Train ---\n",
        "i_train = log_start(\"train_model\")\n",
        "print(\"Training model...\")\n",
        "t0 = utcnow()\n",
        "model.fit(X_train, y_train)\n",
        "t1 = utcnow()\n",
        "print(f\"Model trained in {(t1 - t0).total_seconds():.1f} sec\")\n",
        "log_end(i_train)\n",
        "\n",
        "\n",
        "# Predict\n",
        "i_pred = log_start(\"predict\")\n",
        "print(\"Predicting...\")\n",
        "t0 = utcnow()\n",
        "y_pred = model.predict(X_test)\n",
        "t1 = utcnow()\n",
        "print(f\"Prediction done in {(t1 - t0).total_seconds():.1f} sec\")\n",
        "log_end(i_pred)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Buffering 10s before evaluation to capture RAM samples\")\n",
        "time.sleep(10)\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "i_eval = log_start(\"evaluate\")\n",
        "print(\"Evaluating...\")\n",
        "valid = (~y_test.isna()) & (~pd.isna(y_pred))\n",
        "y_true = y_test[valid].to_numpy()\n",
        "y_hat  = y_pred[valid]\n",
        "if len(y_true) == 0:\n",
        "    print(\"No valid rows to evaluate (missing target).\")\n",
        "    mse = rmse = mae = r2 = float(\"nan\")\n",
        "else:\n",
        "    mse  = mean_squared_error(y_true, y_hat)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae  = mean_absolute_error(y_true, y_hat)\n",
        "    r2   = r2_score(y_true, y_hat)\n",
        "\n",
        "\n",
        "print(\"Buffering 10s after evaluation to capture RAM samples\")\n",
        "time.sleep(10)\n",
        "log_end(i_eval)\n",
        "\n",
        "\n",
        "# Printed metrics for manual note-taking\n",
        "print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "print(f\"MSE : {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAE : {mae:.2f}\")\n",
        "\n",
        "\n",
        "print(\"\\n=== STEP TIMINGS (pretty) ===\")\n",
        "print(as_timing_df().to_string(index=False))\n",
        "print(\"\\n=== Workflow end ===\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBHvzXR8lmQ1",
        "outputId": "854ac4d8-6bf9-42dd-8c5b-659ceea97559"
      },
      "outputs": [],
      "source": [
        "# === Block 3: Export timing to CSV (UTC ISO-8601) ===\n",
        "# Produces one row per (process, phase), where phase in {\"start\",\"end\"}.\n",
        "# Columns: process, phase, time\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "if \"data\" not in globals() or not data:\n",
        "    raise RuntimeError(\"No timing data found. Run Block 2 first.\")\n",
        "\n",
        "def rows_from_step(step):\n",
        "    return [\n",
        "        {\"process\": step[\"process\"], \"phase\": \"start\", \"time\": step[\"start\"].strftime(\"%Y-%m-%dT%H:%M:%SZ\")},\n",
        "        {\"process\": step[\"process\"], \"phase\": \"end\",   \"time\": step[\"end\"].strftime(\"%Y-%m-%dT%H:%M:%SZ\")},\n",
        "    ]\n",
        "\n",
        "save_rows = []\n",
        "for step in data:\n",
        "    if step[\"start\"] is None or step[\"end\"] is None:\n",
        "        continue\n",
        "    save_rows.extend(rows_from_step(step))\n",
        "\n",
        "df_out = pd.DataFrame(save_rows)\n",
        "df_out.to_csv(\"/content/data.csv\", index=False)\n",
        "print(\"Saved timing data to /content/data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "id": "qqWSugFkl72R",
        "outputId": "9d53a35e-608c-465d-cca5-756f11bfaf7e"
      },
      "outputs": [],
      "source": [
        "# === Block 4: RAM plots actual + normalized time ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# ---- Inputs ----\n",
        "TIMING_CSV = \"/content/data.csv\"               # written by Block 3\n",
        "LOG_PATH   = \"/content/process_ram_usage.txt\"  # written by Block 1\n",
        "\n",
        "\n",
        "# ---- Output base (per-run folder) ----\n",
        "RUNS_BASE = \"/content/drive/MyDrive/Dissertation/runs\"\n",
        "os.makedirs(RUNS_BASE, exist_ok=True)\n",
        "\n",
        "\n",
        "# ---- Fixed segment colors (blue, green, yellow, red) ----\n",
        "COLOR_LOADING  = \"#1f77b4\"  # blue\n",
        "COLOR_JOIN     = \"#2ca02c\"  # green\n",
        "COLOR_TRAINING = \"#ffd700\"  # yellow (gold)\n",
        "COLOR_EVAL     = \"#d62728\"  # red\n",
        "\n",
        "\n",
        "# ---- Segment layout for normalized plot ----\n",
        "SEG_WIDTHS = {\"loading\": 0.20, \"join\": 0.20, \"training\": 0.40, \"evaluate\": 0.20}\n",
        "SEG_ORDER = [\"loading\", \"join\", \"training\", \"evaluate\"]\n",
        "SEG_COLORS = {\n",
        "    \"loading\": COLOR_LOADING,\n",
        "    \"join\": COLOR_JOIN,\n",
        "    \"training\": COLOR_TRAINING,\n",
        "    \"evaluate\": COLOR_EVAL,\n",
        "}\n",
        "\n",
        "\n",
        "# 1) Load workflow events (expects: process, phase, time)\n",
        "events = pd.read_csv(TIMING_CSV)\n",
        "events[\"time\"] = pd.to_datetime(events[\"time\"], utc=True)\n",
        "\n",
        "\n",
        "first_start = events.loc[events[\"phase\"] == \"start\", \"time\"].min()\n",
        "if pd.isna(first_start):\n",
        "    raise RuntimeError(\"No start event found in timing CSV.\")\n",
        "\n",
        "\n",
        "# Stable run_id per run\n",
        "run_id = \"run_\" + first_start.strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "run_dir = os.path.join(RUNS_BASE, run_id)\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Milestones required by segments\n",
        "markers = [{\"label\": \"before_load\", \"time\": first_start}]\n",
        "label_map = {\n",
        "    \"load_test\": \"after_load_test\",\n",
        "    \"join\": \"after_join\",\n",
        "    \"train_model\": \"after_train\",\n",
        "    \"evaluate\": \"after_eval\",\n",
        "}\n",
        "for proc, lab in label_map.items():\n",
        "    row = events[(events[\"process\"] == proc) & (events[\"phase\"] == \"end\")]\n",
        "    if not row.empty:\n",
        "        markers.append({\"label\": lab, \"time\": pd.to_datetime(row.iloc[0][\"time\"], utc=True)})\n",
        "markers_df = pd.DataFrame(markers).sort_values(\"time\").reset_index(drop=True)\n",
        "\n",
        "\n",
        "# 2) Parse RAM log (SYSTEM_USED bytes to GB)\n",
        "def parse_ram_log(path=LOG_PATH):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"RAM log not found: {path}\")\n",
        "    ts = None\n",
        "    rows = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if s.startswith(\"TS \"):\n",
        "                ts = pd.to_datetime(s.split()[1], utc=True)\n",
        "            elif s.startswith(\"SYSTEM_USED\"):\n",
        "                try:\n",
        "                    used_bytes = int(s.split()[1])\n",
        "                    if ts is not None:\n",
        "                        rows.append({\"time\": ts, \"ram_gb\": used_bytes / (1024**3)})\n",
        "                except Exception:\n",
        "                    pass\n",
        "    df = pd.DataFrame(rows).sort_values(\"time\").reset_index(drop=True)\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Parsed RAM log is empty. Has the logger run long enough?\")\n",
        "    return df\n",
        "\n",
        "\n",
        "ram = parse_ram_log()\n",
        "ram[\"time\"] = pd.to_datetime(ram[\"time\"], utc=True)\n",
        "\n",
        "\n",
        "# 3) Align milestones to nearest sample at-or-before\n",
        "markers_for_merge = markers_df.rename(columns={\"time\": \"event_time\"}).sort_values(\"event_time\")\n",
        "aligned_df = pd.merge_asof(\n",
        "    markers_for_merge, ram.sort_values(\"time\"),\n",
        "    left_on=\"event_time\", right_on=\"time\", direction=\"backward\"\n",
        ").rename(columns={\"time\":\"plot_time\"})[[\"label\",\"event_time\",\"plot_time\",\"ram_gb\"]]\n",
        "aligned_df = aligned_df.dropna(subset=[\"plot_time\"]).reset_index(drop=True)\n",
        "m_time = {r.label: r.plot_time for _, r in aligned_df.iterrows()}\n",
        "m_y    = {r.label: r.ram_gb    for _, r in aligned_df.iterrows()}\n",
        "\n",
        "\n",
        "# 4) Consolidated segments (4 with join)\n",
        "SEGMENTS = [\n",
        "    (\"loading\",  \"before_load\",    \"after_load_test\"),\n",
        "    (\"join\",     \"after_load_test\",\"after_join\"),\n",
        "    (\"training\", \"after_join\",     \"after_train\"),\n",
        "    (\"evaluate\", \"after_train\",    \"after_eval\"),\n",
        "]\n",
        "\n",
        "\n",
        "# Helpers\n",
        "def interp_ram_at(t, times, values):\n",
        "    if t <= times.iloc[0]:  return float(values.iloc[0])\n",
        "    if t >= times.iloc[-1]: return float(values.iloc[-1])\n",
        "    idx = times.searchsorted(t, side=\"left\")\n",
        "    t0, t1 = times.iloc[idx-1], times.iloc[idx]\n",
        "    y0, y1 = values.iloc[idx-1], values.iloc[idx]\n",
        "    frac = (t - t0) / (t1 - t0)\n",
        "    return float(y0 + frac * (y1 - y0))\n",
        "\n",
        "\n",
        "def build_segment_series(t_start, t_end, ram_df):\n",
        "    ram_df = ram_df.sort_values(\"time\").reset_index(drop=True)\n",
        "    if t_end <= t_start:\n",
        "        return pd.DataFrame(columns=[\"time\",\"ram_gb\"])\n",
        "    inside = ram_df[(ram_df[\"time\"] > t_start) & (ram_df[\"time\"] < t_end)].copy()\n",
        "    y_start = interp_ram_at(t_start, ram_df[\"time\"], ram_df[\"ram_gb\"])\n",
        "    y_end   = interp_ram_at(t_end,   ram_df[\"time\"], ram_df[\"ram_gb\"])\n",
        "    out = pd.concat([\n",
        "        pd.DataFrame([{\"time\": t_start, \"ram_gb\": y_start}]),\n",
        "        inside[[\"time\",\"ram_gb\"]],\n",
        "        pd.DataFrame([{\"time\": t_end,   \"ram_gb\": y_end}]),\n",
        "    ], ignore_index=True).sort_values(\"time\").reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "\n",
        "def segment_stats(df):\n",
        "    if len(df) < 2:\n",
        "        return dict(mean=np.nan, peak=df[\"ram_gb\"].max() if not df.empty else np.nan,\n",
        "                    duration_secs=0.0, samples=len(df))\n",
        "    t = df[\"time\"].astype(\"int64\").to_numpy() / 1e9  # seconds\n",
        "    y = df[\"ram_gb\"].to_numpy()\n",
        "    duration = t[-1] - t[0]\n",
        "    area = np.trapezoid(y, t)  # np.trapz deprecated\n",
        "    mean = area / duration if duration > 0 else np.nan\n",
        "    peak = float(np.max(y))\n",
        "    return dict(mean=mean, peak=peak, duration_secs=duration, samples=len(df))\n",
        "\n",
        "\n",
        "# 5) Build series & stats\n",
        "seg_series, seg_stats = {}, {}\n",
        "for seg_name, start_lab, end_lab in SEGMENTS:\n",
        "    if (start_lab in m_time) and (end_lab in m_time):\n",
        "        t0, t1 = m_time[start_lab], m_time[end_lab]\n",
        "        if t1 > t0:\n",
        "            s = build_segment_series(t0, t1, ram)\n",
        "            seg_series[seg_name] = s\n",
        "            seg_stats[seg_name] = segment_stats(s)\n",
        "\n",
        "\n",
        "avail_times = [m_time[l] for l in [\"before_load\",\"after_load_test\",\"after_join\",\"after_train\",\"after_eval\"] if l in m_time]\n",
        "if len(avail_times) >= 2:\n",
        "    run_start, run_end = min(avail_times), max(avail_times)\n",
        "else:\n",
        "    run_start = first_start\n",
        "    run_end   = ram[\"time\"].iloc[-1]\n",
        "run_series = build_segment_series(run_start, run_end, ram)\n",
        "overall_stats = segment_stats(run_series)\n",
        "\n",
        "\n",
        "# 6) Plot 1 : Actual time (UTC)\n",
        "fig1, ax1 = plt.subplots(figsize=(14, 6))\n",
        "ax1.plot(ram[\"time\"], ram[\"ram_gb\"], linewidth=2, alpha=0.95, zorder=3)\n",
        "y_min, y_max = ram[\"ram_gb\"].min(), ram[\"ram_gb\"].max()\n",
        "y_text = y_min + 0.05 * (y_max - y_min)\n",
        "for seg_name, start_lab, end_lab in SEGMENTS:\n",
        "    if seg_name in seg_series:\n",
        "        df = seg_series[seg_name]\n",
        "        ax1.fill_between(df[\"time\"], 0, df[\"ram_gb\"], color=SEG_COLORS[seg_name], alpha=0.28, zorder=1)\n",
        "        x0, x1 = df[\"time\"].iloc[0], df[\"time\"].iloc[-1]\n",
        "        y0, y1 = df[\"ram_gb\"].iloc[0], df[\"ram_gb\"].iloc[-1]\n",
        "        ax1.scatter([x0, x1], [y0, y1], s=80, zorder=4, color=SEG_COLORS[seg_name], edgecolors=\"black\")\n",
        "        label = \"Loading\" if seg_name==\"loading\" else (\"Join\" if seg_name==\"join\" else (\"Training\" if seg_name==\"training\" else \"Evaluate\"))\n",
        "        x_mid = x0 + (x1 - x0) / 2\n",
        "        ax1.text(x_mid, y_text, label, ha=\"center\", va=\"center\",\n",
        "                 fontsize=10, fontweight=\"bold\",\n",
        "                 bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.6),\n",
        "                 zorder=5)\n",
        "ax1.set_title(\"Memory Profile: SCIKIT LEARN (DecisionTreeRegressor) — Join of Tables in Memory (Full Dataset)\", fontweight=\"bold\")\n",
        "ax1.set_xlabel(\"Time (UTC)\")\n",
        "ax1.set_ylabel(\"Memory Usage (GB)\")\n",
        "ax1.grid(True, alpha=0.3, linestyle=\"--\")\n",
        "ax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\", tz=mdates.UTC))\n",
        "fig1.autofmt_xdate()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 7) Plot 2 : Normalized time\n",
        "seg_spans, cursor = {}, 0.0\n",
        "for name in SEG_ORDER:\n",
        "    w = SEG_WIDTHS[name]\n",
        "    seg_spans[name] = (cursor, cursor + w)\n",
        "    cursor += w\n",
        "\n",
        "\n",
        "norm_rows = []\n",
        "for seg_name in SEG_ORDER:\n",
        "    if seg_name not in seg_series:\n",
        "        continue\n",
        "    df = seg_series[seg_name]\n",
        "    t0, t1 = df[\"time\"].iloc[0], df[\"time\"].iloc[-1]\n",
        "    denom_ns = (t1.value - t0.value)\n",
        "    if denom_ns <= 0:\n",
        "        continue\n",
        "    times_ns = df[\"time\"].astype(\"int64\").to_numpy()\n",
        "    x_seg = (times_ns - t0.value) / float(denom_ns)  # [0,1] within segment\n",
        "    span0, span1 = seg_spans[seg_name]\n",
        "    x_global = span0 + x_seg * (span1 - span0)       # [0,1] with 20/20/40/20 widths\n",
        "    for xs, xg, y, tt in zip(x_seg, x_global, df[\"ram_gb\"].to_numpy(), df[\"time\"]):\n",
        "        norm_rows.append({\n",
        "            \"run_id\": run_id, \"segment\": seg_name,\n",
        "            \"x_segment_norm\": float(xs), \"x_global_norm\": float(xg),\n",
        "            \"ram_gb\": float(y), \"time_utc\": tt.isoformat()\n",
        "        })\n",
        "norm_df = pd.DataFrame(norm_rows).sort_values([\"x_global_norm\"]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Save minimal per-run data for super plot\n",
        "norm_csv_path = os.path.join(run_dir, \"ram_profile_normalized.csv\")\n",
        "norm_df.to_csv(norm_csv_path, index=False)\n",
        "\n",
        "\n",
        "# Plot 2: Normalized RAM\n",
        "fig2, ax2 = plt.subplots(figsize=(14, 6))\n",
        "if not norm_df.empty:\n",
        "    ax2.plot(norm_df[\"x_global_norm\"], norm_df[\"ram_gb\"], linewidth=2, alpha=0.95, zorder=3)\n",
        "    for seg_name in SEG_ORDER:\n",
        "        span = seg_spans[seg_name]\n",
        "        df_seg = norm_df[norm_df[\"segment\"] == seg_name]\n",
        "        if df_seg.empty:\n",
        "            continue\n",
        "        ax2.fill_between(df_seg[\"x_global_norm\"], 0, df_seg[\"ram_gb\"],\n",
        "                         color=SEG_COLORS[seg_name], alpha=0.28, zorder=1)\n",
        "        x0, x1 = span\n",
        "        y0 = df_seg.iloc[(df_seg[\"x_global_norm\"]-x0).abs().argmin()][\"ram_gb\"]\n",
        "        y1 = df_seg.iloc[(df_seg[\"x_global_norm\"]-x1).abs().argmin()][\"ram_gb\"]\n",
        "        ax2.scatter([x0, x1], [y0, y1], s=80, zorder=4, color=SEG_COLORS[seg_name], edgecolors=\"black\")\n",
        "        label = \"Loading\" if seg_name==\"loading\" else (\"Join\" if seg_name==\"join\" else (\"Training\" if seg_name==\"training\" else \"Evaluate\"))\n",
        "        x_mid = x0 + (x1 - x0) / 2\n",
        "        y_min2, y_max2 = norm_df[\"ram_gb\"].min(), norm_df[\"ram_gb\"].max()\n",
        "        y_text2 = y_min2 + 0.05 * (y_max2 - y_min2)\n",
        "        ax2.text(x_mid, y_text2, label, ha=\"center\", va=\"center\",\n",
        "                 fontsize=10, fontweight=\"bold\",\n",
        "                 bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.6),\n",
        "                 zorder=5)\n",
        "ax2.set_xlim(0.0, 1.0)\n",
        "ax2.set_title(\"Memory Profile: SCIKIT LEARN (DecisionTreeRegressor) — Join of Tables in Memory (Full Dataset)\", fontweight=\"bold\")\n",
        "ax2.set_xlabel(\"Normalized Time\")\n",
        "ax2.set_ylabel(\"Memory Usage (GB)\")\n",
        "ax2.grid(True, alpha=0.3, linestyle=\"--\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 8) Print and save summary stats (overall + per segment)\n",
        "def pretty_stats(name, st):\n",
        "    print(f\"{name:>10}: mean={st['mean']:.3f} GB | peak={st['peak']:.3f} GB | duration={st['duration_secs']:.1f}s | samples={st['samples']}\")\n",
        "print(\"\\n=== RAM SUMMARY (time-weighted) ===\")\n",
        "pretty_stats(\"OVERALL\", overall_stats)\n",
        "for seg_name in SEG_ORDER:\n",
        "    if seg_name in seg_stats:\n",
        "        title = \"LOADING\" if seg_name==\"loading\" else (\"JOIN\" if seg_name==\"join\" else (\"TRAINING\" if seg_name==\"training\" else \"EVALUATE\"))\n",
        "        pretty_stats(title, seg_stats[seg_name])\n",
        "\n",
        "\n",
        "summary_rows = [{\n",
        "    \"run_id\": run_id, \"scope\": \"overall\",\n",
        "    \"mean_ram_gb\": overall_stats[\"mean\"], \"peak_ram_gb\": overall_stats[\"peak\"],\n",
        "    \"duration_secs\": overall_stats[\"duration_secs\"], \"samples\": overall_stats[\"samples\"]\n",
        "}]\n",
        "for seg_name in SEG_ORDER:\n",
        "    if seg_name in seg_stats:\n",
        "        summary_rows.append({\n",
        "            \"run_id\": run_id, \"scope\": seg_name,\n",
        "            \"mean_ram_gb\": seg_stats[seg_name][\"mean\"], \"peak_ram_gb\": seg_stats[seg_name][\"peak\"],\n",
        "            \"duration_secs\": seg_stats[seg_name][\"duration_secs\"], \"samples\": seg_stats[seg_name][\"samples\"]\n",
        "        })\n",
        "pd.DataFrame(summary_rows).to_csv(os.path.join(run_dir, \"run_summary.csv\"), index=False)\n",
        "print(f\"\\n Saved normalized plot data: {norm_csv_path}\")\n",
        "print(f\" Saved summary stats:       {os.path.join(run_dir, 'run_summary.csv')}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
