{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# SUPER PLOT \n",
    "# ================================================================\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "# ---------- INPUT ROOT / NAMING ----------\n",
    "BASE_ROOT   = r\"C:\\Monitor\\all\"\n",
    "RUN_PREFIX  = \"ASUTOSH_20250818-\"  \n",
    "CSV_NAME    = \"DataCollector01.csv\"\n",
    "\n",
    "# Use decimal GB (1000^3) or GiB (1024^3)\n",
    "USE_DECIMAL_GB = False\n",
    "\n",
    "# ---------- FIXED SEGMENT SPANS & COLORS (60/10/30) ----------\n",
    "SEG_SPANS  = {\n",
    "    \"training\":  (0.00, 0.60),  # 60%\n",
    "    \"idle\":      (0.60, 0.70),  # 10%\n",
    "    \"evaluate\":  (0.70, 1.00),  # 30%\n",
    "}\n",
    "SEG_COLORS = {\n",
    "    \"training\":  \"#ffd700\",   # yellow\n",
    "    \"idle\":      \"#9e9e9e\",   # gray\n",
    "    \"evaluate\":  \"#d62728\",   # red\n",
    "}\n",
    "\n",
    "def _gb_divisor():\n",
    "    return (1000.0**3) if USE_DECIMAL_GB else (1024.0**3)\n",
    "\n",
    "def _run_csv_path(n: int) -> str:\n",
    "    folder = f\"{RUN_PREFIX}{n:06d}\"\n",
    "    return os.path.join(BASE_ROOT, folder, CSV_NAME)\n",
    "\n",
    "def read_pdh_sum_working_set(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a PDH-CSV 4.0 file and compute, for each row:\n",
    "      total_app = sum over all columns matching Process(*)\\\\Working Set\n",
    "    Returns DataFrame ['timestamp','app_gb'] sorted by time.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    df = pd.read_csv(path, header=0)\n",
    "    ts_col = df.columns[0]\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[ts_col], errors=\"coerce\")\n",
    "\n",
    "    ws_cols = [c for c in df.columns if (\"Process(\" in c and \"Working Set\" in c)]\n",
    "    if not ws_cols:\n",
    "        raise ValueError(f\"No Process(*)\\\\Working Set columns in {os.path.basename(path)}\")\n",
    "\n",
    "    ws_bytes = df[ws_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    total_bytes = ws_bytes.sum(axis=1)\n",
    "\n",
    "    out = (pd.DataFrame({\n",
    "            \"timestamp\": df[\"timestamp\"],\n",
    "            \"app_gb\": total_bytes / _gb_divisor()\n",
    "          })\n",
    "          .dropna()\n",
    "          .sort_values(\"timestamp\")\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "    if out.empty:\n",
    "        raise ValueError(f\"No valid rows after parsing in {os.path.basename(path)}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "def normalize_segment(df: pd.DataFrame, seg_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map a segment's timestamps onto its fixed normalized span.\n",
    "    Returns DataFrame with ['x_global_norm','app_gb','segment'].\n",
    "    \"\"\"\n",
    "    a, b = SEG_SPANS[seg_name]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"x_global_norm\",\"app_gb\",\"segment\"])\n",
    "\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    t0, t1 = df[\"timestamp\"].iloc[0], df[\"timestamp\"].iloc[-1]\n",
    "    t0_ns = int(pd.to_datetime(t0).value)\n",
    "    t1_ns = int(pd.to_datetime(t1).value)\n",
    "    denom = (t1_ns - t0_ns)\n",
    "\n",
    "    if denom <= 0:\n",
    "        # Single-sample or zero-span: flat line across the segment\n",
    "        return pd.DataFrame({\n",
    "            \"x_global_norm\": [a, b],\n",
    "            \"app_gb\":        [df[\"app_gb\"].iloc[0], df[\"app_gb\"].iloc[0]],\n",
    "            \"segment\":       seg_name\n",
    "        })\n",
    "\n",
    "    times_ns = df[\"timestamp\"].astype(\"int64\").to_numpy()\n",
    "    x_seg = (times_ns - times_ns[0]) / float(times_ns[-1] - times_ns[0])  # [0,1] within segment\n",
    "    x_global = a + x_seg * (b - a)\n",
    "    return pd.DataFrame({\n",
    "        \"x_global_norm\": x_global,\n",
    "        \"app_gb\":        df[\"app_gb\"].to_numpy(),\n",
    "        \"segment\":       seg_name\n",
    "    })\n",
    "\n",
    "# ---------- Build 5 (training, evaluation) pairs\n",
    "pairs = [(i, i+1) for i in range(6, 15, 2)]\n",
    "\n",
    "all_runs = []\n",
    "valid_runs = 0\n",
    "\n",
    "for idx, (n_train, n_eval) in enumerate(pairs, start=1):\n",
    "    train_csv = _run_csv_path(n_train)\n",
    "    eval_csv  = _run_csv_path(n_eval)\n",
    "    try:\n",
    "        train_df = read_pdh_sum_working_set(train_csv)\n",
    "        eval_df  = read_pdh_sum_working_set(eval_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP set {idx}] {e}\")\n",
    "        continue\n",
    "\n",
    "    # ----- Build a 2-point Idle \"bridge\" (end of training -> start of evaluation)\n",
    "    t_end_time   = train_df.iloc[-1][\"timestamp\"]\n",
    "    t_end_val    = float(train_df.iloc[-1][\"app_gb\"])\n",
    "    e_start_time = eval_df.iloc[0][\"timestamp\"]\n",
    "    e_start_val  = float(eval_df.iloc[0][\"app_gb\"])\n",
    "\n",
    "    idle_df = pd.DataFrame({\n",
    "        \"timestamp\": [t_end_time, e_start_time],\n",
    "        \"app_gb\":    [t_end_val,  e_start_val],\n",
    "    }).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # ----- Normalize to 60/10/30 spans\n",
    "    d_train = normalize_segment(train_df, \"training\")\n",
    "    d_idle  = normalize_segment(idle_df,  \"idle\")\n",
    "    d_eval  = normalize_segment(eval_df,  \"evaluate\")\n",
    "\n",
    "    d_run = pd.concat([d_train, d_idle, d_eval], ignore_index=True)\n",
    "    d_run[\"run_id\"] = f\"set_{idx:02d}\"\n",
    "    all_runs.append(d_run)\n",
    "    valid_runs += 1\n",
    "\n",
    "if not all_runs:\n",
    "    raise RuntimeError(\"No valid runs found. Check folder names and CSV contents.\")\n",
    "\n",
    "all_df = (pd.concat(all_runs, ignore_index=True)\n",
    "          .sort_values([\"run_id\",\"x_global_norm\"])\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "# ---------- Determine rows processed per run (labels & colors) ----------\n",
    "unique_runs = sorted(all_df[\"run_id\"].unique())\n",
    "\n",
    "# Manual overrides if you want exact counts (absolute rows)\n",
    "# Example: {\"set_01\": 100_000_000, \"set_02\": 80_000_000, ...}\n",
    "RUN_ROWS_HINT = {}\n",
    "\n",
    "rows_map = {}\n",
    "\n",
    "# 1) Use manual hints if provided\n",
    "for rid, v in RUN_ROWS_HINT.items():\n",
    "    rows_map[rid] = int(v)\n",
    "\n",
    "# 2) Try to parse millions from the source folder names if they contain '100m', '80M', etc.\n",
    "#    We look up the underlying numbered folders used for this run_id.\n",
    "#    (Optional heuristic — safe to skip if your folders don't encode size.)\n",
    "def _parse_size_from_folder(num: int) -> int | None:\n",
    "    folder = f\"{RUN_PREFIX}{num:06d}\"\n",
    "    m = re.search(r'(\\d{2,3})\\s*[mM]\\b', folder)\n",
    "    return int(m.group(1)) * 1_000_000 if m else None\n",
    "\n",
    "for idx, rid in enumerate(unique_runs, start=1):\n",
    "    if rid in rows_map:\n",
    "        continue\n",
    "    # The training folder number used for this set is 4,6,8,10,12 + (idx-1)*2 in this scheme,\n",
    "    # but since we already built the data above, we just try a heuristic parse:\n",
    "    # (No-op if nothing matches.)\n",
    "    # You can ignore this block; fallback below will cover the labels.\n",
    "    pass\n",
    "\n",
    "# 3) Fallback: distribute ~100M → 20M across all runs descending\n",
    "missing = [rid for rid in unique_runs if rid not in rows_map]\n",
    "if missing:\n",
    "    n = len(unique_runs)\n",
    "    approx_millions = np.linspace(100, 20, n)  # e.g., [100, 80, 60, 40, 20]\n",
    "    approx_map = {rid: int(round(mm)) * 1_000_000 for rid, mm in zip(unique_runs, approx_millions)}\n",
    "    for rid in missing:\n",
    "        rows_map[rid] = approx_map[rid]\n",
    "\n",
    "# ---------- SUPER PLOT: overlay all normalized runs ----------\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Background bands (training yellow, idle gray, evaluation red)\n",
    "for seg, (a, b) in SEG_SPANS.items():\n",
    "    ax.axvspan(a, b, color=SEG_COLORS[seg], alpha=0.12, zorder=0)\n",
    "\n",
    "# Visual boundaries at 0.60 and 0.70\n",
    "ax.axvline(SEG_SPANS[\"training\"][1], color=\"black\", linewidth=1, alpha=0.35, linestyle=\"--\", zorder=1)  # 0.60\n",
    "ax.axvline(SEG_SPANS[\"idle\"][1],     color=\"black\", linewidth=1, alpha=0.35, linestyle=\"--\", zorder=1)  # 0.70\n",
    "\n",
    "# Color runs by dataset size (bigger = darker) + bolder lines\n",
    "min_rows = min(rows_map.values())\n",
    "max_rows = max(rows_map.values())\n",
    "norm = Normalize(vmin=min_rows, vmax=max_rows)\n",
    "cmap = get_cmap(\"viridis\")\n",
    "\n",
    "# Sort runs by size (largest first) for nice layering\n",
    "runs_sorted = sorted(unique_runs, key=lambda r: rows_map[r], reverse=True)\n",
    "\n",
    "for rid in runs_sorted:\n",
    "    d = all_df[all_df[\"run_id\"] == rid].sort_values(\"x_global_norm\")\n",
    "    label = f\"{rows_map[rid]/1e6:.0f}M rows\"\n",
    "    ax.plot(\n",
    "        d[\"x_global_norm\"], d[\"app_gb\"],\n",
    "        linewidth=2.25,           # ← bolder lines\n",
    "        alpha=0.85,\n",
    "        color=cmap(norm(rows_map[rid])),\n",
    "        label=label,\n",
    "        zorder=2\n",
    "    )\n",
    "\n",
    "# Bottom labels\n",
    "y_min, y_max = all_df[\"app_gb\"].min(), all_df[\"app_gb\"].max()\n",
    "y_text = y_min + 0.05 * (y_max - y_min) if np.isfinite(y_min) and y_min != y_max else y_min\n",
    "for seg, (a, b) in SEG_SPANS.items():\n",
    "    ax.text(a + (b-a)/2, y_text,\n",
    "            \"Training\" if seg == \"training\" else (\"Idle\" if seg == \"idle\" else \"Evaluation\"),\n",
    "            ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.6),\n",
    "            zorder=3)\n",
    "\n",
    "# Titles & axes\n",
    "ax.set_xlim(0, 1)\n",
    "unit_label = \"GB\" if USE_DECIMAL_GB else \"GB\"\n",
    "ax.set_title(f\"InDatabase (CART) -Regression Tree Model\", fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Normalized Time\")\n",
    "ax.set_ylabel(f\"Memory Usage ({unit_label})\")\n",
    "ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# Legend INSIDE (top-left), de-duplicated by size label\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "seen = set(); h_unique = []; l_unique = []\n",
    "for h, l in zip(handles, labels):\n",
    "    if l not in seen:\n",
    "        h_unique.append(h); l_unique.append(l); seen.add(l)\n",
    "\n",
    "leg = ax.legend(\n",
    "    h_unique, l_unique,\n",
    "    title=\"Rows processed\",\n",
    "    loc=\"upper right\",bbox_to_anchor=(0.99, 0.99),\n",
    "    frameon=True, fancybox=True, borderaxespad=0.0,\n",
    "    handlelength=2.4\n",
    ")\n",
    "leg.get_frame().set_alpha(0.85)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plotted {valid_runs} run(s) out of 5.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# ---------- FIXED SEGMENT COLORS ----------\n",
    "COLOR_TRAINING = \"#ffd700\"   # yellow (gold)\n",
    "COLOR_EVAL     = \"#d62728\"   # red\n",
    "COLOR_IDLE     = \"#9e9e9e\"   # neutral gray\n",
    "SEG_COLORS = {\"Training\": COLOR_TRAINING, \"Idle\": COLOR_IDLE, \"Evaluation\": COLOR_EVAL}\n",
    "\n",
    "# ---------- NORMALIZED SECTION SPANS (60/10/30) ----------\n",
    "SEG_SPANS = {\n",
    "    \"Training\":   (0.00, 0.60),  # 60%\n",
    "    \"Idle\":       (0.60, 0.70),  # 10%\n",
    "    \"Evaluation\": (0.70, 1.00),  # 30%\n",
    "}\n",
    "\n",
    "# ---------- INPUT: update paths if your filenames differ ----------\n",
    "folder  = r\"C:\\Monitor\\all\\ASUTOSH_20250824-000016\"   # training\n",
    "folder1 = r\"C:\\Monitor\\all\\ASUTOSH_20250824-000017\"   # evaluation\n",
    "train_csv_path = os.path.join(folder,  \"DataCollector01.csv\")\n",
    "eval_csv_path  = os.path.join(folder1, \"DataCollector01.csv\")\n",
    "\n",
    "USE_DECIMAL_GB = False   # set True if you prefer GB (1000^3)\n",
    "def _gb_divisor(): return (1000.0**3) if USE_DECIMAL_GB else (1024.0**3)\n",
    "\n",
    "def read_pdh_sum_working_set(path: str):\n",
    "    \"\"\"\n",
    "    Read a PDH-CSV 4.0 file and compute, for each row:\n",
    "      total_app_gb = sum over all columns matching Process(*)\\Working Set\n",
    "    Returns DataFrame with ['timestamp','app_gb'] sorted by time.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    df = pd.read_csv(path, header=0)\n",
    "    ts_col = df.columns[0]\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[ts_col], errors=\"coerce\")\n",
    "\n",
    "    ws_cols = [c for c in df.columns if (\"Process(\" in c and \"Working Set\" in c)]\n",
    "    if not ws_cols:\n",
    "        raise ValueError(f\"No Process(... )\\\\Working Set columns found in {os.path.basename(path)}\")\n",
    "\n",
    "    ws_bytes = df[ws_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    total_bytes = ws_bytes.sum(axis=1)\n",
    "\n",
    "    out = (pd.DataFrame({\n",
    "            \"timestamp\": df[\"timestamp\"],\n",
    "            # Multiply by 10 for decimal place consistency\n",
    "            \"app_gb\": (total_bytes / _gb_divisor()) * 10.0\n",
    "        })\n",
    "        .dropna()\n",
    "        .sort_values(\"timestamp\")\n",
    "        .reset_index(drop=True))\n",
    "\n",
    "    if out.empty:\n",
    "        raise ValueError(f\"No valid rows after parsing in {os.path.basename(path)}\")\n",
    "\n",
    "    return out, ws_cols\n",
    "\n",
    "train_app, train_cols = read_pdh_sum_working_set(train_csv_path)\n",
    "eval_app,  eval_cols  = read_pdh_sum_working_set(eval_csv_path)\n",
    "\n",
    "t_end_time, t_end_val     = train_app.iloc[-1][\"timestamp\"], float(train_app.iloc[-1][\"app_gb\"])\n",
    "e_start_time, e_start_val = eval_app.iloc[0][\"timestamp\"],  float(eval_app.iloc[0][\"app_gb\"])\n",
    "\n",
    "idle_app = pd.DataFrame({\n",
    "    \"timestamp\": [t_end_time, e_start_time],\n",
    "    \"app_gb\":    [t_end_val,  e_start_val],\n",
    "}).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "app_all = (pd.concat([train_app, idle_app, eval_app], ignore_index=True)\n",
    "           .drop_duplicates(subset=[\"timestamp\"])\n",
    "           .sort_values(\"timestamp\")\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "segments = [\n",
    "    (\"Training\",   train_app.iloc[0][\"timestamp\"], train_app.iloc[-1][\"timestamp\"]),\n",
    "    (\"Idle\",       t_end_time,                     e_start_time),\n",
    "    (\"Evaluation\", eval_app .iloc[0][\"timestamp\"], eval_app .iloc[-1][\"timestamp\"]),\n",
    "]\n",
    "\n",
    "def segment_stats(df: pd.DataFrame):\n",
    "    if df.empty:\n",
    "        return dict(mean=np.nan, peak=np.nan, duration_secs=0.0, samples=0)\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    if len(df) == 1:\n",
    "        y = float(df[\"app_gb\"].iloc[0])\n",
    "        return dict(mean=y, peak=y, duration_secs=0.0, samples=1)\n",
    "    t = df[\"timestamp\"].astype(\"int64\").to_numpy() / 1e9  # seconds\n",
    "    y = df[\"app_gb\"].to_numpy(dtype=float)\n",
    "    duration = float(t[-1] - t[0])\n",
    "    area = float(np.trapz(y, t))\n",
    "    mean = area / duration if duration > 0 else np.nan\n",
    "    peak = float(np.nanmax(y))\n",
    "    return dict(mean=mean, peak=peak, duration_secs=duration, samples=len(df))\n",
    "\n",
    "def normalize_segment(df: pd.DataFrame, seg_label: str) -> pd.DataFrame:\n",
    "    a, b = SEG_SPANS[seg_label]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"x_global_norm\",\"app_gb\",\"segment\"])\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    if len(df) == 1:\n",
    "        return pd.DataFrame({\"x_global_norm\":[a, b], \"app_gb\":[df[\"app_gb\"].iloc[0]]*2, \"segment\":seg_label})\n",
    "    t = df[\"timestamp\"].astype(\"int64\").to_numpy()\n",
    "    x_seg = (t - t[0]) / float(t[-1] - t[0])  # [0,1] within the segment\n",
    "    x_global = a + x_seg * (b - a)\n",
    "    return pd.DataFrame({\"x_global_norm\": x_global, \"app_gb\": df[\"app_gb\"].to_numpy(float), \"segment\": seg_label})\n",
    "\n",
    "train_stats = segment_stats(train_app)\n",
    "eval_stats  = segment_stats(eval_app)\n",
    "overall_stats = segment_stats(app_all)\n",
    "\n",
    "norm_train = normalize_segment(train_app, \"Training\")\n",
    "norm_idle  = normalize_segment(idle_app,  \"Idle\")\n",
    "norm_eval  = normalize_segment(eval_app,  \"Evaluation\")\n",
    "norm_df = (pd.concat([norm_train, norm_idle, norm_eval], ignore_index=True)\n",
    "           .sort_values(\"x_global_norm\")\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "# ===================== PLOT 1: ACTUAL TIME ======================\n",
    "fig1, ax1 = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "ax1.plot(app_all[\"timestamp\"], app_all[\"app_gb\"], linewidth=2, alpha=0.95, zorder=3)\n",
    "ax1.fill_between(app_all[\"timestamp\"], 0, app_all[\"app_gb\"], alpha=0.08, zorder=1)\n",
    "\n",
    "y_min, y_max = app_all[\"app_gb\"].min(), app_all[\"app_gb\"].max()\n",
    "y_text = y_min + 0.06 * (y_max - y_min) if np.isfinite(y_min) and y_min != y_max else y_min\n",
    "for label, s, e in segments:\n",
    "    if e <= s:\n",
    "        continue\n",
    "    seg = app_all[(app_all[\"timestamp\"] >= s) & (app_all[\"timestamp\"] <= e)]\n",
    "    if seg.empty:\n",
    "        continue\n",
    "    color = SEG_COLORS.get(label, \"#cccccc\")\n",
    "    ax1.fill_between(seg[\"timestamp\"], 0, seg[\"app_gb\"], color=color, alpha=0.28, zorder=2)\n",
    "    ax1.scatter([seg.iloc[0][\"timestamp\"], seg.iloc[-1][\"timestamp\"]],\n",
    "                [seg.iloc[0][\"app_gb\"],    seg.iloc[-1][\"app_gb\"]],\n",
    "                s=80, zorder=4, color=color, edgecolors=\"black\")\n",
    "    ax1.text(s + (e - s)/2, y_text, label,\n",
    "             ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\",\n",
    "             bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.6),\n",
    "             zorder=5)\n",
    "\n",
    "ax1.set_title(f\"Memory Profile: Join-Aware In-Database Regression Tree (Full Dataset)\",\n",
    "              fontweight=\"bold\")\n",
    "ax1.set_xlabel(\"Time\")\n",
    "unit_label = \"GB\"\n",
    "ax1.set_ylabel(f\"Memory Usage ({unit_label})\")\n",
    "ax1.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n",
    "ax1.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.1f'))  # one decimal\n",
    "\n",
    "fig1.autofmt_xdate()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =================== PLOT 2: NORMALIZED TIME ====================\n",
    "fig2, ax2 = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "for lbl, (a, b) in SEG_SPANS.items():\n",
    "    ax2.axvspan(a, b, color=SEG_COLORS[lbl], alpha=0.12, zorder=0)\n",
    "\n",
    "ax2.axvline(SEG_SPANS[\"Training\"][1], color=\"black\", linewidth=1, alpha=0.35, linestyle=\"--\", zorder=1)\n",
    "ax2.axvline(SEG_SPANS[\"Idle\"][1],     color=\"black\", linewidth=1, alpha=0.35, linestyle=\"--\", zorder=1)\n",
    "\n",
    "ax2.plot(norm_df[\"x_global_norm\"], norm_df[\"app_gb\"], linewidth=2, alpha=0.95, zorder=3)\n",
    "\n",
    "y_min2, y_max2 = norm_df[\"app_gb\"].min(), norm_df[\"app_gb\"].max()\n",
    "y_text2 = y_min2 + 0.06 * (y_max2 - y_min2) if np.isfinite(y_min2) and y_min2 != y_max2 else y_min2\n",
    "\n",
    "for lbl in [\"Training\", \"Idle\", \"Evaluation\"]:\n",
    "    a, b = SEG_SPANS[lbl]\n",
    "    df_seg = norm_df[(norm_df[\"x_global_norm\"] >= a) & (norm_df[\"x_global_norm\"] <= b)]\n",
    "    if df_seg.empty:\n",
    "        continue\n",
    "    color = SEG_COLORS[lbl]\n",
    "    ax2.fill_between(df_seg[\"x_global_norm\"], 0, df_seg[\"app_gb\"], color=color, alpha=0.28, zorder=2)\n",
    "    x0, x1 = a, b\n",
    "    y0 = df_seg.iloc[(df_seg[\"x_global_norm\"]-x0).abs().argmin()][\"app_gb\"]\n",
    "    y1 = df_seg.iloc[(df_seg[\"x_global_norm\"]-x1).abs().argmin()][\"app_gb\"]\n",
    "    ax2.scatter([x0, x1], [y0, y1], s=80, zorder=4, color=color, edgecolors=\"black\")\n",
    "    ax2.text(a + (b - a)/2, y_text2, lbl,\n",
    "             ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\",\n",
    "             bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.6),\n",
    "             zorder=5)\n",
    "\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_title(f\"Memory Profile: Join-Aware In-Database Regression Tree (Full Dataset)\",\n",
    "              fontweight=\"bold\")\n",
    "ax2.set_xlabel(\"Normalized Time\")\n",
    "unit_label = \"GB\"\n",
    "ax2.set_ylabel(f\"Memory Usage ({unit_label})\")\n",
    "ax2.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "ax2.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.1f'))  # one decimal\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ======================= Console summary ========================\n",
    "def pretty_stats(name, st):\n",
    "    print(f\"{name:>10}: mean={st['mean']:.1f} {unit_label} | \"\n",
    "          f\"peak={st['peak']:.1f} {unit_label} | \"\n",
    "          f\"duration={st['duration_secs']:.1f}s | samples={st['samples']}\")\n",
    "\n",
    "print(\"\\n=== RAM SUMMARY (time-weighted) ===\")\n",
    "pretty_stats(\"OVERALL\",   overall_stats)\n",
    "pretty_stats(\"TRAINING\",  train_stats)\n",
    "pretty_stats(\"EVALUATION\",eval_stats)\n",
    "\n",
    "print(\"\\nParsed Working Set columns (training):\", len(train_cols))\n",
    "print(\"Parsed Working Set columns (evaluation):\", len(eval_cols))\n",
    "print(\"Median totals — TRAIN(GB):\", f\"{float(np.nanmedian(train_app['app_gb'])):.1f}\",\n",
    "      \"| EVAL(GB):\", f\"{float(np.nanmedian(eval_app['app_gb'])):.1f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
